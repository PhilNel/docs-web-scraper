{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Web Scraper Lambda","text":"<p>This documentation outlines the architecture and design of the Lambda functions used to fetch rendered HTML and parse it into structured job listings for storage in downstream systems.</p> <p>Explore the tabs to see how it works.</p>"},{"location":"#related-repositories","title":"Related Repositories","text":"<p>\ud83d\udce6 perl-web-scraper - Contains the Perl Lambda function that parses job listings from raw HTML using Mojo::DOM.</p> <p>\ud83d\udce6 go-web-scraper \u2013 Contains the Go-based Lambda function that parses job listings from raw HTML using goquery.</p> <p>\ud83d\udce6 node-web-fetcher - Headless browser Lambda that uses Puppeteer to fetch fully rendered HTML from dynamic websites.</p> <p>\ud83d\udce6 terraform-web-scraper - Terraform configuration for deploying the infrastructure and managing Lambda permissions, S3 buckets, etc.</p>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>This project implements a two-phase web scraping pipeline using AWS Lambda functions written in Node.js, Go and Perl. The system is designed to handle dynamic, JavaScript-rendered pages and convert them into structured data.</p>"},{"location":"architecture/overview/#high-level-flow","title":"High-Level Flow","text":"<p>This diagram is generated from YAML using <code>awsdac</code>.</p> <p></p>"},{"location":"architecture/overview/#components","title":"Components","text":"<ol> <li> <p>fetcher-schedule</p> <ul> <li>An EventBridge rule that triggers the <code>fetcher-lambda</code> on a defined schedule (e.g., daily)</li> <li>Allows full decoupling of the fetching cadence from other processing components</li> </ul> </li> <li> <p>fetcher-lambda</p> <ul> <li>A Node.js Lambda function responsible for fetching HTML content</li> <li>Uses headless Chromium (via Puppeteer) to render JavaScript-heavy pages</li> <li>Stores the fully rendered HTML in an intermediate S3 bucket</li> </ul> </li> <li> <p>web-scraper-output</p> <ul> <li>Acts as a bridge between the fetch and parse phases</li> <li>Receives raw HTML from the fetcher</li> </ul> </li> <li> <p>s3-parser-trigger</p> <ul> <li>An EventBridge rule that listens for <code>ObjectCreated</code> events from the <code>web-scraper-output</code> S3 bucket</li> <li>Allows for decoupled and scalable downstream processing</li> </ul> </li> <li> <p>parser-lambda</p> <ul> <li>A Perl-based Lambda function and a Go Lambda function that parse the stored HTML</li> <li>The Perl function uses a custom runtime implemented via a <code>bootstrap</code> script and is deployed as a Docker container, while the Go function uses a zip-based deployment with the <code>provided.al2</code> runtime</li> <li>Parses HTML using <code>Mojo::DOM</code> (Perl) or <code>PuerkitoBio/goquery</code> (Go) to extract structured job listings</li> </ul> </li> <li> <p>job-store</p> <ul> <li>Stores structured job listings parsed by the Lambda functions</li> </ul> </li> </ol>"},{"location":"architecture/overview/#infrastructure","title":"Infrastructure","text":"<ul> <li>terraform-web-scraper Terraform project for managing infrastructure: S3 buckets, Lambda deployment, IAM roles, and CI/CD wiring.</li> </ul>"},{"location":"features/html-fetching/","title":"HTML Fetching","text":"<p>The HTML fetching phase is responsible for retrieving dynamic, JavaScript-rendered web pages and storing the fully rendered HTML in an S3 bucket for downstream parsing.</p>"},{"location":"features/html-fetching/#purpose","title":"Purpose","text":"<p>Many job listing sites (and other dynamic content sources) rely on client-side JavaScript to render their content. Traditional HTTP fetches or curl-style requests are insufficient because they return only the raw HTML shell, not the complete page. Our fetcher solves this by executing the JavaScript and extracting the fully rendered DOM.</p>"},{"location":"features/html-fetching/#architecture","title":"Architecture","text":"<ul> <li>Implemented as an AWS Lambda function and written in Node.js</li> <li>Uses Puppeteer in combination with <code>sparticuz/chrome-aws-lambda</code> to run headless Chromium</li> <li>Outputs the rendered HTML to an S3 bucket (<code>web-scraper-output</code>)</li> </ul>"},{"location":"features/html-fetching/#design-rationale","title":"Design Rationale","text":""},{"location":"features/html-fetching/#why-nodejs","title":"Why Node.js?","text":"<ul> <li>\u2705 Availability of a precompiled, Lambda-optimized Chromium binary via <code>sparticuz/chrome-aws-lambda</code></li> <li>\u2705 Integration with Puppeteer for headless browsing and DOM evaluation</li> <li>\u274c Perl was not selected due to the lack of modern headless browser libraries and prebuilt Chromium support</li> </ul>"},{"location":"features/html-fetching/#why-puppeteer","title":"Why Puppeteer?","text":"<ul> <li>Provides full browser context (JS execution, cookies, network hooks)</li> <li>Battle-tested for scraping, automation, and rendering use cases</li> <li>Easily configurable to run in headless mode, with timeout control and page event hooks</li> </ul>"},{"location":"features/html-fetching/#why-use-s3-as-an-intermediate-step","title":"Why use S3 as an intermediate step?","text":"<ul> <li>\u2705 Decouples the fetch and parse stages, allowing each to be triggered, retried, and debugged independently</li> <li>\u2705 Makes raw HTML available for inspection or reuse without re-running Puppeteer</li> </ul>"},{"location":"features/html-fetching/#current-limitations","title":"Current Limitations","text":"<ul> <li>No built-in retry or exponential backoff\u2014failures are logged and discarded</li> <li>Currently stores all output to a flat S3 namespace; tagging or metadata may be needed later</li> </ul>"},{"location":"features/html-fetching/#future-enhancements","title":"Future Enhancements","text":"<ul> <li>Add support for fetching multiple pages/sites in a single execution</li> <li>Integrate error reporting and retry logic for failed fetches</li> </ul>"},{"location":"features/html-parsing/","title":"HTML Parsing","text":"<p>The HTML parsing phase extracts structured data from previously fetched and rendered HTML documents. It is implemented as a Perl-based Lambda function using a custom runtime and is designed to convert raw DOM content into job listings.</p> <p>In addition to the Perl-based parser, a Go-based equivalent Lambda function was also developed. This was done with the intention of comparing development effort and performance between the two implementations.</p>"},{"location":"features/html-parsing/#purpose","title":"Purpose","text":"<p>After the fetcher Lambda stores raw HTML snapshots in S3, this phase consumes them and extracts job data (e.g. title, company, location, link). By isolating this responsibility, we can iterate on parsing logic independently of how the content is retrieved.</p>"},{"location":"features/html-parsing/#architecture","title":"Architecture","text":""},{"location":"features/html-parsing/#perl-lambda","title":"Perl Lambda","text":"<ul> <li>Implemented as an AWS Lambda function using a custom runtime</li> <li>Deployed as a Docker container to support the custom Perl runtime and required dependencies</li> <li>Uses <code>Mojo::DOM</code> for HTML parsing</li> </ul>"},{"location":"features/html-parsing/#go-lambda","title":"Go Lambda","text":"<ul> <li>Implemented as an AWS Lambda function using the provided.al2 (Amazon Linux 2) runtime</li> <li>Compiled as a statically-linked binary and packaged into a <code>.zip</code> for Lambda deployment</li> <li>Uses <code>PuerkitoBio/goquery</code> (jQuery-like syntax) for HTML parsing</li> </ul>"},{"location":"features/html-parsing/#common","title":"Common","text":"<ul> <li>Reads HTML files from S3 (<code>web-scraper-output</code>)</li> <li>Outputs structured job listings to DynamoDB (<code>job-store</code>)</li> </ul>"},{"location":"features/html-parsing/#design-rationale","title":"Design Rationale","text":""},{"location":"features/html-parsing/#why-perl","title":"Why Perl?","text":"<ul> <li>\u2705 Chosen as a learning objective: to understand modern Perl usage in backend contexts</li> <li>\u2705 Enables experimentation with AWS Lambda\u2019s custom runtime API</li> <li>\u2705 Perl offers mature regex and DOM parsing capabilities with minimal dependencies</li> <li>\u274c No official Lambda runtime support (requires custom lambda handler)</li> </ul>"},{"location":"features/html-parsing/#why-go","title":"Why Go?","text":"<ul> <li>\u2705 Familiarity with Go \u2014 The developer is more experienced with Go, allowing for faster iteration and debugging</li> <li>\u2705 Performance comparison \u2014 Parsing performance and memory usage can be benchmarked between Perl and Go to determine optimal runtime characteristics</li> <li>\u2705 Cross-language learning \u2014 Maintaining both versions showcases how different ecosystems approach clean architecture, HTML parsing, and AWS integration</li> <li>\u274c Duplication of effort since both implementations must be maintained</li> </ul>"},{"location":"features/html-parsing/#why-mojodom","title":"Why Mojo::DOM?","text":"<ul> <li>Lightweight, pure-Perl HTML DOM parser</li> <li>Supports CSS selectors, text extraction, and attribute traversal</li> </ul>"},{"location":"features/html-parsing/#why-goquery","title":"Why goquery?","text":"<ul> <li>Lightweight and easy to use Go HTML parser</li> <li>Built on top of the Go standard library\u2019s HTML tokenizer, ensuring performance and reliability</li> <li>Widely used and well-documented</li> </ul>"},{"location":"features/html-parsing/#why-a-custom-runtime","title":"Why a custom runtime?","text":"<ul> <li>AWS does not natively support Perl as a Lambda runtime</li> <li>To understand the lambda runtime for future unsupported runtimes</li> </ul>"},{"location":"features/html-parsing/#why-paws-instead-of-amazons3-amazondynamodb","title":"Why Paws (instead of Amazon::S3 / Amazon::DynamoDB)?","text":"<p>We initially chose Amazon::S3 and Amazon::DynamoDB for their minimal dependency footprint and quick setup. However, we've now migrated to Paws, the official Perl AWS SDK, for the following reasons:</p> <ul> <li>\u2705 Supports temporary credentials, which are required for execution in AWS Lambda environments (unlike Amazon::DynamoDB, which lacks this support)</li> <li>\u2705 Unified SDK for multiple AWS services, simplifying future integrations</li> <li>\u2705 More consistent and complete API coverage for AWS services</li> </ul> <p>The disadvantages of this decision are as listed below:</p> <ul> <li>\u26a0\ufe0f Larger Docker image size, as Paws introduces more dependencies</li> <li>\u26a0\ufe0f Slower CI builds, especially for linting and unit tests</li> </ul> <p>However, with AWS Lambda now supporting larger deployment packages (up to 10 GB with container images), the increased size is an acceptable trade-off for improved functionality and long-term maintainability.</p>"},{"location":"features/html-parsing/#output-flexibility","title":"Output Flexibility","text":"<p>The parser-lambdas support pluggable \"sink\" implementations to handle different output targets:</p> <ul> <li>\u2705 DynamoDB \u2014 Currently supported. Parsed job listings are stored as structured records for easy querying and downstream consumption.</li> <li>\u2705 Console output \u2014 A minimal sink used for debugging and local development.</li> <li>\ud83d\udfe1 S3 support \u2014 Planned. This would allow raw or structured job data to be archived in object storage (e.g., as newline-delimited JSON).</li> </ul> <p>This design enables new sinks (e.g., for S3 or external APIs) to be added without modifying parser internals.</p>"},{"location":"features/html-parsing/#multi-site-support-and-dynamic-parser-selection","title":"Multi-site Support and Dynamic Parser Selection","text":"<p>The parsing Lambda supports multiple job listing websites through simple S3 key naming convention.</p> <p>Each HTML file uploaded by the fetcher Lambda is stored in S3 with a prefix that corresponds to the source company as follows:</p> <pre><code>s3://web-scraper-output-dev-af-south-1/\n\u251c\u2500\u2500 duckduckgo/\n\u2502   \u2514\u2500\u2500 rendered.html\n\u251c\u2500\u2500 posthog/\n\u2502   \u2514\u2500\u2500 rendered.html\n</code></pre> <p>When the parser Lambda is invoked via an EventBridge notification from S3, it receives the object key (e.g. <code>duckduckgo/rendered.html</code>) in the event payload. </p> <p>The handler extracts the first path segment of the S3 key (in this case, <code>duckduckgo</code>) to:</p> <ul> <li>Identify which company the job listings belong to</li> <li>Instantiate the appropriate parser implementation for that site's HTML structure</li> </ul> <p>This allows the system to support additional job listing websites by:</p> <ul> <li>Updating the fetcher config to include the new site's URL</li> <li>Implementing a new parser module that conforms to the parser interface</li> <li>Registering the new parser in the parser factory</li> </ul>"},{"location":"features/html-parsing/#current-limitations","title":"Current Limitations","text":"<ul> <li>Assumes all HTML files are consistent in structure</li> <li>Minimal error handling for broken or malformed HTML</li> </ul>"},{"location":"features/html-parsing/#future-enhancements","title":"Future Enhancements","text":"<ul> <li>Extract the custom runtime implementation into a reusable CPAN module for use in other Perl-based Lambdas</li> </ul>"}]}